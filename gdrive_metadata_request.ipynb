{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import dependencies \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "\n",
    "import pygsheets\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate dictionary of parent folders\n",
    "\n",
    "#for this example, there is only one folder in the list\n",
    "folderList = {'FOLDER NAME' : 'FOLDER ID'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### variation on code to return list of files\n",
    "\n",
    "# define scope - by convention this variable is SCOPES\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n",
    "\n",
    "# function to return list of files \n",
    "# paramater is length of list\n",
    "def getFileList():\n",
    "\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials2.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    # Connect to the API service\n",
    "    try:\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    except HttpError as error:\n",
    "        # TODO(developer) - Handle errors from drive API.\n",
    "        print(f'An error occurred: {error}')\n",
    "  \n",
    "    page_token = None\n",
    "    dict = {}\n",
    "    listCount = 0\n",
    "    while True:\n",
    "        last = list(folderList.values())[listCount]\n",
    "        query = \"'\" + last + \"' in parents\"\n",
    "        print(listCount, last)\n",
    "        response = service.files().list(q=query,\n",
    "                                            spaces='drive',\n",
    "                                            fields='nextPageToken, files(name, id, parents)',\n",
    "                                            pageToken=page_token).execute()\n",
    "        for file in response.get('files', []):\n",
    "            # Process change\n",
    "            dict.update({file.get('name'):[file.get('id'),file.get('parents')]})\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        listCount = listCount + 1\n",
    "        if listCount == len(folderList):\n",
    "            return dict\n",
    "\n",
    "df = pd.DataFrame.from_dict(getFileList(),orient='index',columns=['id','parent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up DataFrame of files, ids, and parents\n",
    "\n",
    "df1 = df\n",
    "df1['parent'] = df1['parent'].str.join(', ')\n",
    "df2 = df1.reset_index()\n",
    "df2.head()#.iloc[:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to get the name of all sheets in a file\n",
    "\n",
    "def getAllSheets(dfOfFiles,iCount):\n",
    "    gc = pygsheets.authorize(service_file='pygsheets.json')\n",
    "    # open google spreadsheet\n",
    "    sh = gc.open(dfOfFiles.iloc[0:,0][iCount])\n",
    "    # define which sheet to open by sheet name\n",
    "    wks = sh.worksheets()\n",
    "    return(wks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### funtion to transform list of pygsheets worksheets into a list of strings\n",
    "\n",
    "def dfOfWks(wks):\n",
    "    itemCount = 0\n",
    "    listOf = []\n",
    "    #df = pd.DataFrame()\n",
    "    while True:\n",
    "        sheetName = wks[itemCount].title\n",
    "        listOf.append(sheetName)\n",
    "        itemCount += 1\n",
    "        if itemCount == len(wks):\n",
    "            return(listOf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to generate DataFrame with all sheets in a given file\n",
    "\n",
    "def dfOfWkbks(wBook,wSheet,iCount):\n",
    "    dfWBook = pd.DataFrame(columns=['name','id','parent','sheet_name'])\n",
    "    dfWBook = dfWBook.assign(file_name=lambda x: wSheet)\n",
    "    dfWBook = dfWBook.assign(name=lambda x: wBook['index'].iloc[iCount])\n",
    "    dfWBook = dfWBook.assign(id=lambda x: wBook['id'].iloc[iCount])\n",
    "    dfWBook = dfWBook.assign(parent=lambda x: wBook['parent'].iloc[iCount])\n",
    "    return(dfWBook)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to getnerate DataFrame with all sheets in all files\n",
    "\n",
    "def makeList(dfOfFiles):\n",
    "    iCount = 0\n",
    "    df40 = pd.DataFrame()\n",
    "    while True:\n",
    "        df10 = getAllSheets(dfOfFiles,iCount)\n",
    "        df20 = dfOfWks(df10)\n",
    "        df30 = dfOfWkbks(dfOfFiles,df20,iCount)\n",
    "        df40 = pd.concat([df40,df30])\n",
    "        iCount += 1\n",
    "        if iCount == len(dfOfFiles):\n",
    "            return(df40)\n",
    "\n",
    "dfFinal = makeList(df2)\n",
    "dfFinal.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### output file name list for use in next stage\n",
    "\n",
    "dfFinal.to_csv('files_list.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49e53e0ddad2980e35f979ab131222550b6caf82d4f4f17a9b066eafc1fafab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit ('01_ts_code_directory-Xf6xDVFD': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
