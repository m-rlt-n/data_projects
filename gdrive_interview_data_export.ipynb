{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import dependencies \n",
    "\n",
    "# data mgmt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# dates\n",
    "from datetime import date\n",
    "\n",
    "# google sheets\n",
    "import pygsheets\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql.cursors \n",
    "import cryptography\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pull associate ids (THIS BE REPLACED BY A QUERY TO THE AF DEV REDSHIFT)\n",
    "\n",
    "# environ variables for db access\n",
    "db_user = os.environ.get('USER')\n",
    "db_password = os.environ.get('PASS')\n",
    "db_host = os.environ.get('HOST')\n",
    "db_port = 25060\n",
    "db = 'database'\n",
    "\n",
    "# connect to ts database\n",
    "connection = pymysql.connect(host=db_host,\n",
    "                             user=db_user,\n",
    "                             password=db_password,\n",
    "                             port=db_port,\n",
    "                             database=db,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "# create cursor\n",
    "cursor=connection.cursor()\n",
    "\n",
    "# select all associate ID data\n",
    "sql = \"SELECT * FROM database.medadata;\"\n",
    "cursor.execute(sql)\n",
    "meta_df = pd.DataFrame(cursor.fetchall())\n",
    "\n",
    "# commit + close connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET snapFile TO THE NAME OF THE FILE YOU WANT TO USE \n",
    "\n",
    "# create variable for today's date\n",
    "today = date.today()\n",
    "\n",
    "# create a filename including today's date\n",
    "snapFile = 'file_list' + str(today) + '.csv' \n",
    "# IF YOU ARE RUNNING gdrive_metadata_request.ipynb WITH A file_list_[date].csv FROM \n",
    "# DATE OTHER THAN today(), YOU WILL NEED TO MANUALLY EDIT THE VALUE OF snapFile\n",
    "# this step is required to ensure that you are working with the right data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read snapshot file meta-data from csv\n",
    "\n",
    "# this file is creatded using the gdrive_metadata_request.ipynb script\n",
    "# it includes a number of manual steps to identify sheets for import \n",
    "# and to attach employee IDs - open to a more automated approach but \n",
    "# did not develop one given the inconsistencies in the data\n",
    "\n",
    "# read snapFile\n",
    "try:\n",
    "    df = pd.read_csv(snapFile)\n",
    "except FileNotFoundError as error:\n",
    "        print(f'check your snapFile \\nFileNotFoundError: {error}')\n",
    "\n",
    "# df.shape # check your work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### select only the files we want to read\n",
    "\n",
    "# use the 'use' column to create a boolean mask of files we want\n",
    "dfmask = df['USE'].isin([1])\n",
    "snapDf = df[dfmask]\n",
    "\n",
    "# reset index on snapDf so that pullData can itterate through it\n",
    "snapDf = snapDf.reset_index()\n",
    "snapDf = snapDf.drop('index',axis=1)\n",
    "\n",
    "# snapDf.head(2) # check your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to pull data from google sheets specified in snapDf\n",
    "\n",
    "# in order to work, the service account referecned in service_file \n",
    "# will need to be permissioned to each google sheet listed in snapDf\n",
    "def pullData(fileName,sheet,headers,values):\n",
    "    # authorization\n",
    "    gc = pygsheets.authorize(service_file='pygsheets.json')\n",
    "\n",
    "    # open the spreadsheet\n",
    "    sh = gc.open(fileName)#af_file)\n",
    "\n",
    "    # define which sheet to open by sheet name\n",
    "    wks = sh.worksheet('title',sheet)#af_sheets[0])\n",
    "    vals = wks.range(values, returnas='matrix')\n",
    "\n",
    "    # pull data into df\n",
    "    data = DataFrame (vals, columns=wks.range(headers, returnas='matrix'))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to standardize the size/ shape of data \n",
    "\n",
    "# generally, the scorecared data should be in columns A:E with \n",
    "# headers in row 4. however for older files, and possilby for \n",
    "# new file formats, this will not be the case\n",
    "def shapeData(data):\n",
    "    if data.shape[1] == 5:\n",
    "        data2 = data\n",
    "    if data.shape[1] == 3:\n",
    "        data2 = data\n",
    "        data2.columns = ['module','score','source_dep']\n",
    "        data2 = data2.assign(competency = lambda x: data2['source_dep'])\n",
    "        data2 = data2.assign(evidence = lambda x: \"\")\n",
    "        data2 = data2[['module','competency','score','source_dep','evidence']]        \n",
    "    else:\n",
    "        data2 = data\n",
    "    return(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to reformat snapshot data\n",
    "\n",
    "def cleanData(dataVal,evalDate,candidateName,assocId,mileStone):#,asc_ids):\n",
    "    dataVal.columns = ['module_dep','competency','score','source_dep','evidence']#,'metric_id']\n",
    "\n",
    "    # clear rows for which there is no 'score'\n",
    "    df_mask = ~dataVal['score'].isin([\"\"])\n",
    "    df2 = dataVal[df_mask]\n",
    "\n",
    "    # metadat for spreading 'module' accross all relevant rows\n",
    "    df2 = df2.assign(mod_name = lambda x: df2['module_dep'].apply(lambda x: 0 if x == \"\" else 1))\n",
    "    df2 = df2.assign(join = lambda x: df2['mod_name'].cumsum())\n",
    "\n",
    "    # temp table to map 'module', 'name' records to join\n",
    "    df_mask2 = df2['mod_name'].isin([1])\n",
    "    df3 = df2[df_mask2]\n",
    "    df3 = df3[['join','module_dep']]\n",
    "    df3.columns = ['index','module']\n",
    "    df3[['name','employee_id','milestone_id']] = [candidateName,assocId,mileStone] #this will iterate through the list once that becomes relevant\n",
    "\n",
    "    # merge df2 and temp table to add 'module', 'name', and 'employee_id'\n",
    "    df4 = df2.merge(df3[['index','module','name','employee_id','milestone_id']],how='left', left_on='join', right_on='index')\n",
    "    df4 = df4[['employee_id','name','module','competency','score','source_dep','evidence','milestone_id']]#'metric_id',\n",
    "\n",
    "    # metadata for spreading 'source' accross all relevant rows\n",
    "    df4['source_name'] = df4['source_dep'].apply(lambda x: 0 if x == \"\" else 1)\n",
    "    df4['join'] = df4['source_name'].cumsum()\n",
    "\n",
    "    # temp table to map 'source' names to join\n",
    "    df_mask4 = df4['source_name'].isin([1])\n",
    "    df5 = df4[df_mask4]\n",
    "    df5 = df5[['join','source_dep']]\n",
    "    df5.columns = ['index','source']\n",
    "\n",
    "    # merge df4 and temp table to add SOURCE, then add dummy columns for evaluator data (we may or may not be able to include these stats)\n",
    "    df6 = df4.merge(df5[['index','source']],how='left', left_on='join', right_on='index') \n",
    "    df6['eval_date'] = pd.to_datetime(evalDate)  \n",
    "    df6[['evaluator','eval_employee_id','source_id']] = ['evaluator','eval_employee_id','S00-CS-0007']\n",
    "    df6 = df6[['eval_date','employee_id','name','module','competency','score','source','evidence','evaluator','eval_employee_id','milestone_id','source_id']]\n",
    "    return(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###function to combine all dataframes\n",
    "\n",
    "def createDf(snapDf):\n",
    "    item_count = 0\n",
    "    output = pd.DataFrame() \n",
    "    while True:\n",
    "        # file specific variables for pullData and cleanData\n",
    "        fileName = snapDf.iloc[:,1][item_count]\n",
    "        sheet = snapDf.iloc[:,4][item_count]\n",
    "        evalDate = snapDf.iloc[:,5][item_count]\n",
    "        mileStone = snapDf.iloc[:,7][item_count]\n",
    "        candidateName = snapDf.iloc[:,9][item_count]\n",
    "        assocId =  snapDf.iloc[:,10][item_count]\n",
    "        headers = snapDf.iloc[:,12][item_count]\n",
    "        values = snapDf.iloc[:,13][item_count]\n",
    "\n",
    "        # print metadata for diagnosing errors\n",
    "        print(item_count, fileName)\n",
    "        print(sheet, assocId, headers, values)\n",
    "\n",
    "        # run functions\n",
    "        data = pullData(fileName,sheet,headers,values)\n",
    "        dataVal = shapeData(data)\n",
    "        out = cleanData(dataVal,evalDate,candidateName,assocId,mileStone) \n",
    "\n",
    "        # append new data to df\n",
    "        output = pd.concat([output,out])\n",
    "        item_count +=1\n",
    "        if item_count == len(snapDf):\n",
    "            break\n",
    "    return(output)\n",
    "\n",
    "scoreDf = createDf(snapDf)\n",
    "scoreDf = scoreDf.astype(object).replace(np.nan, '')\n",
    "\n",
    "# scoreDf.head() # check your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean and standardize data for 'metric_id' join\n",
    "\n",
    "scoreDf2 = scoreDf\n",
    "scoreDf2['module'] = scoreDf2['module'].apply(lambda x: \"module1\" if str(x).__contains__(\"module1\") else x)\n",
    "scoreDf2['module'] = scoreDf2['module'].apply(lambda x: \"module2\" if str(x).__contains__(\"module2\") else x)\n",
    "scoreDf2['module'] = scoreDf2['module'].apply(lambda x: \"module3\" if str(x).__contains__(\"module3\") else x)\n",
    "scoreDf2['module'] = scoreDf2['module'].apply(lambda x: \"module4\" if str(x).__contains__(\"module4\") else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pull in metric ids from external source\n",
    "\n",
    "# bring in metric_id mapping to strings in candidate snapshots\n",
    "dfMetId = pd.read_csv('metric_id_to_string_map.csv')\n",
    "\n",
    "# merge metric ids into snapshot data\n",
    "mergeDf = scoreDf2.merge(dfMetId[['module','competency','metric_id']],how='left', left_on=['module','competency'], right_on=['module','competency'])\n",
    "mergeDf.head(2)\n",
    "\n",
    "# reorder columns and drop unwanted\n",
    "mergeDf2 = mergeDf[['eval_date', 'employee_id', 'name', 'module',\n",
    "                'competency', 'score', 'source', 'evidence', 'evaluator',\n",
    "                'eval_employee_id','metric_id', 'milestone_id', 'source_id']]\n",
    "\n",
    "# fix milestone_ids and clean NaN values for DB\n",
    "mergeDf2['milestone_id'] = mergeDf2['milestone_id'].apply(lambda x: \"0\" + str(x)[:1] if str(x).__contains__(\"0\") else \"0\" + x)\n",
    "mergeDf3 = mergeDf2.astype(object).replace(np.nan, '')\n",
    "\n",
    "### check your work\n",
    "# mergeDf3.head()\n",
    "# snapCheck = 'snapshot_data_' + str(today) + '.csv'\n",
    "# mergeDf3.to_csv(snapCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import values to MySQL db - to be replaved with AF Redshift \n",
    "\n",
    "# connect to ts database\n",
    "connection = pymysql.connect(host=db_host,\n",
    "                             user=db_user,\n",
    "                             password=db_password,\n",
    "                             port=db_port,\n",
    "                             database=db,\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "# create cursor\n",
    "cursor=connection.cursor()\n",
    "\n",
    "# create column list for insertion \n",
    "cols = \"`,`\".join([str(i) for i in mergeDf3.columns.tolist()])\n",
    "\n",
    "# insert DataFrame records one by one \n",
    "for i,row in mergeDf3.iterrows():\n",
    "    sql = \"INSERT INTO `interview_data` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "    cursor.execute(sql, tuple(row))\n",
    "\n",
    "    # connection is not autocommit by default\n",
    "    connection.commit()\n",
    "\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49e53e0ddad2980e35f979ab131222550b6caf82d4f4f17a9b066eafc1fafab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 ('01_ts_code_directory-Xf6xDVFD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
